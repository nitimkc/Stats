\documentclass[a4paper, 11pt]{article}
\usepackage{covington}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[catalan]{babel}
\usepackage{graphicx}
\usepackage{eurosym}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{bm}
\usepackage{layout}
\textheight=23.94cm 
\textwidth=17cm 
\topmargin=-1cm 
\oddsidemargin=-0.5cm 
 
\newcommand{\header}[4]{
	\begin{center}
		\rule{\linewidth}{0.5pt}
		
		{\small{#1}}
      
        \vspace{0.2in}
        
		{\large{#2}}
		
        \vspace{0.2in}
        
		{\small{#3}}
		
		\vspace{0.15in}
		
		{#4}
		
		\vspace{-0.1in}
		\rule{\linewidth}{0.6pt}
	\end{center}
}

\begin{document}
 
\header{\sc Barcelona Graduate School of Economics \hfill Master's Degree in Data Science}{\bf Statistical Modeling and Inference $-$ Problem Set \#4}{\sc Niti Mishra $\cdot$ Miquel Torrens $\cdot$ B\'alint V\'an}{November 9\textsuperscript{th}, 2015}
Solution to proposed exercises.\\
% EXERCISE 1
\newline \textbf{\underline{Exercise 1}}\\
\newline We need to show that $y(\mu + \sigma)$ is a point less than one standard deviation away from the mean of the marginal distribution of $t$, that is:
\begin{eqnarray}
y(\mu + \sigma) \leq \bar{t} + \sqrt{\mathbb{V}[t]} \nonumber
\end{eqnarray}
Given that $x \sim \mathcal{N}(\mu, \sigma^2)$ and $\varepsilon \sim \mathcal{N}(0, \tau^2)$ are assumed to be uncorrelated:
\begin{eqnarray}
\mathbb{E}[t] &=& \mathbb{E}[x + \varepsilon] = \mathbb{E}[x] + \mathbb{E}[\varepsilon] = \mu = \bar{t} \nonumber \\
\mathbb{V}[t] &=& \mathbb{V}[x + \varepsilon] = \mathbb{V}[x] + \mathbb{V}[\varepsilon] = \sigma^2 + \tau^2 \nonumber
\end{eqnarray}
We see that $\mu = \bar{t}$ because given the distribution of its components $t$ is a normally (and thus symmetrically) distributed around its mean, and has the same expected value as $x$. On the other hand:
\begin{eqnarray}
y(\mu + \sigma) = \mathbb{E}[t | x = \mu + \sigma] = \mu + \sigma \nonumber
\end{eqnarray}
And so we would need to show that:
\begin{eqnarray}
y(\mu + \sigma) &\leq& \mu + \sqrt{\mathbb{V}[t]} \nonumber \\
\mu + \sigma &\leq& \mu + \sqrt{\sigma^2 + \tau^2} \nonumber \\
\sigma^2 &\leq& \sigma^2 + \tau^2  \nonumber \\
\tau^2 &\geq& 0 \nonumber
\end{eqnarray}
We know that $\tau^2 \geq 0$ is indeed non-negative, thus proved.\\
% EXERCISE 2
\newpage
\textbf{\underline{Exercise 2}}\\
\newline \underline{Part (a)}\\
\newline We perform the following transformations to the raw data:
\begin{itemize}
\item Convert the field \texttt{HEIGHT} to total amount of inches and name it \texttt{HEIGHT\_I}.
\item Reescale the variable \texttt{SEX} to variable \texttt{MEN}, which takes value 1 if the individual is a man and 0 otherwise.
\item We suppress individuals with a reported weight greater than 500, that is, those with values 998 and 999 which are not valid.
\item We suppress individuals with a reported height greater than 8 feet, that is, those with values 998 and 999 which are not valid.
\item We preventively cut off the individual with highest income, as he reports an income that doubles the second highest income (potentially an outlier).
\item We cut off individuals with no income or with income only declared approximately, i.e. we contemplate valid answers in the variable \texttt{EARN1}.
\end{itemize}
Additionally, a first approach was to unify the fields \texttt{EARN1} and \texttt{EARN2} in one single field, adding an extra dummy field named \texttt{INEXACT} that captured whether we use the precise answer from \texttt{EARN1} or the approximated answer in \texttt{EARN2}. The following results were slightly better and we had a bigger sample, but the interpretability of the model became much less intuitive and prediction lost sense, thus we finally decided to work with only precise declared outcomes. This is also the approach taken by the book.\\
\newline \underline{Part (b)}\\
\newline The regression run is the following:\\
\includegraphics[scale=0.7]{reg1.png}
\newline The transformation needed to interpret the intercept as average earnings for people with average height is substracting the mean from the \texttt{HEIGHT\_I} variable. If we do so the resulting intercept is the following:\\
\includegraphics[scale=0.7]{reg2.png}
\newline This states that a person with average height will earn on average an income of approximately \$23,219.\\
\newline \underline{Part (c)}\\
\newline We have run the following models:\\
\newline \\
\includegraphics[scale=0.6]{reg3.png}
\newline \\
\newline We choose model \texttt{m08} for several reasons. First, the residual standard error is the smallest between within log-linear models; second, it has a relatively high $R^2$; third, the regressors are mostly significant and have an intuitive sign, and, fourth, the use of centered variables will help the interpretation of the coefficients. The results are the following:\\
\newline \\
\includegraphics[scale=0.7]{reg4.png}
\newline \\
\newline \underline{Part (d)}\\
\newline The results are only for strictly positive income individuals. The interpretation of the coefficients is the following:
\begin{itemize}
\item The intercept says that a woman of average height and weight is expected to earn $\exp{(9.5129)} \approx \$13,533$.
\item The coefficient for \texttt{HEIGHT\_I\_ST} says that for an average weight woman exceeding the average height by one standard deviation increases the expected income by 10.8\%. Exceeding by two standard deviations would increase the expected salary by 21.6\%, and so on. In the case of the average weight men, this is diminished by the coefficient of \texttt{HEIGHT\_I\_ST:MEN}, which sets the total increase for men to 1.4\% when one standard deviation away, although this coefficient is not significant, so in fact it is likely that there is no different effect for men and women whatsoever.
\item The coefficient for \texttt{WEIGHT\_ST} says that for an average height woman exceeding the average weight by one standard deviation decreases the expected income by 9.7\%. In the case of average height men, this is actually overturned by the coefficient on \texttt{WEIGHT\_ST:MEN}, and the aggregated effect is of +14.4\% on income, so the data shows a negative effect on women and positive one on men.
\item The coefficient on \texttt{MEN} suggests that an man of average height and weight earns on average 41.7\% more than a woman of average height and weight.
\end{itemize}
% EXERCISE 3
\newpage
\textbf{\underline{Exercise 3}}\\
\newline \underline{Part (a)}\\
\newline We generate random draws and plot the result:
\begin{center}
\includegraphics[scale=0.6]{plot_ex3dot1.png}
\end{center}
\underline{Part (b)}\\
\newline We take the sample ratio of the simulations we have drawn and we compute the quantiles of this sample ratio distribution, keeping those that concentrate the confidence percentage that we desire.\\
\newline For the 50\% case, we take quantiles 25\% and 75\%, leaving 50\% of the sample inside (50\% confidence interval). In our sample this interval is $(108.7, 313.7)$.\\
\newline For the 95\% case, we take quantiles 2.5\% and 97.5\%, leaving 95\% of the sample inside (95\% confidence interval). The interval is $(-57.3, 677.9)$.\\
\newline \underline{Part (b)}\\
\newline We change the standard error and we obtain the following results:
\begin{center}
\includegraphics[scale=0.6]{plot_ex3dot3.png}
\end{center}
The intervals are computed analogously are the following:
\begin{itemize}
\item 50\% confidence interval: $(71.2, 326.9)$.
\item 95\% confidence interval: $(-1386.8, 2122.8)$.
\end{itemize}
% EXERCISE 4
\newpage
\textbf{\underline{Exercise 4}}\\
\newline We plot a histogram the district share of democratic votes data for 1988:
\begin{center}
\includegraphics[scale=0.5]{plot_ex4_1.png}
\end{center}
Districts with less than 10\% share are condensed towards zero and districts with more than 90\% are condensed towards one, as they represent uncontested districts.\\
\newline We will try to run a linear regression model that predicts the democratic share of vote in 1988 given the share of vote from the previous election and the sign of the party of the incumbent until the election.
\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{plot_ex4_2.png}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{plot_ex4_3.png}
\end{subfigure}
\end{figure}
In the first plot, bolded squares are districts with Republican incumbents in 1988, white squares belong to Democratic incumbents and crosses represent open seats. The line represented is the 45 degree line. In the second plot we show the same picture but correcting for uncontested wins in 1986, adjusting the shares to 75\%-25\% in favor of the 1986 uncontested winner.\\
\newline The regression is run using only the contested wins in 1988 using the adjusted data. The incumbent variable is coded as follows: $-1$ for Republican holder, $1$ for Democratic holder and $0$ for open seat. The summary of the model is the following:\\
\newline \\
\includegraphics[scale=0.7]{plot_ex4_4.png}
\newline \\
\newline Now, with these results we would like to estimate the outcome for the following election in 1990. To do this, we use 1,000 predictive simulations drawn with the book's \texttt{sim()} function. We multiply the new data (share of votes of 1988 and incumbent as of 1990) and we multiply it with the coefficients of the predictive simulations (that is, $\mathbf{\Phi}_{90} \mathbf{w}$) and we add normally distributed errors.\\
\newline The results of this simulations show for each district (total of 435) what will the Democratic share of votes be for each simulation (total of 1,000), thatt is, a $1000 \times 435$ matrix. The districts won uncontested in 1990 have values \texttt{NA} as the model does not predict them. We set these to zero.\\
\newline The number of elections won by the Democrats in 1990 is predicted using the simple rule of win in case the predicted value is above 50\% and lose otherwise. The results are shown in the book's Figure 7.5, which we approximately replicate here, showing the prediction for the first five districts and the 10 first simulations:
\begin{center}
\includegraphics[scale=0.7]{plot_ex4_5.png}
\end{center}
Gelman and Hill show that the average total wins predicted was actually way off the final result, which was 260, more than three standard deviations away from the mean, so this model does not really look applicable to 1990.\\
\newline In the final part of 7.3 they also try to find a way to find the probability of extreme rare events, and they look to find the probability of a tie. We use their approach to put this theory into practice, and use the means and standard deviations of the simulations of the outcome to predict what is the probability of the actual result falling within the interval $0.5 \pm 1 / (2 n_i)$, where $n_i$ is the number of votes in each district (we use the votes from 1988 in the prediction of 1990). As expected, almost for all districts this probability is extremly small. The histogram of the results is the following:
\begin{center}
\includegraphics[scale=0.7]{plot_ex4_6.png}
\end{center}

\end{document}





